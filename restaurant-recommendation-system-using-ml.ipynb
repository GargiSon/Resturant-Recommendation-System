{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010635,
     "end_time": "2021-03-27T12:17:05.926275",
     "exception": false,
     "start_time": "2021-03-27T12:17:05.915640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## How the Restaurant Recommendation System Works?\n",
    "\n",
    "The rapid growth in data collection has led to a new era of a data-driven world. Data is used to create more efficient systems and thatâ€™s where recommender systems come in.\n",
    "\n",
    "Recommendation systems are a type of information filtering systems because they improve the quality of search results and provide elements that are more relevant to the search item or that are related to the search history of the user.\n",
    "\n",
    "These are active information filtering systems that personalize the information provided to a user based on their interests, relevance of the information, etc. Recommendation systems are widely used to recommend movies, items, restaurants, places to visit, items to buy, etc.\n",
    "\n",
    "There are two types of recommendation systems:\n",
    "\n",
    "1. Content-based filtering\n",
    "2. Collaborative filtering\n",
    "\n",
    "![](https://i0.wp.com/thecleverprogrammer.com/wp-content/uploads/2020/11/1-recommendation.png?resize=1024%2C627&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008922,
     "end_time": "2021-03-27T12:17:05.944822",
     "exception": false,
     "start_time": "2021-03-27T12:17:05.935900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I will start the task of Restaurant Recommendation System by importing the necessary Python Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-27T12:17:05.967888Z",
     "iopub.status.busy": "2021-03-27T12:17:05.967191Z",
     "iopub.status.idle": "2021-03-27T12:17:08.237314Z",
     "shell.execute_reply": "2021-03-27T12:17:08.236015Z"
    },
    "papermill": {
     "duration": 2.283055,
     "end_time": "2021-03-27T12:17:08.237671",
     "exception": false,
     "start_time": "2021-03-27T12:17:05.954616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011342,
     "end_time": "2021-03-27T12:17:08.266463",
     "exception": false,
     "start_time": "2021-03-27T12:17:08.255121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, I will load and read the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T12:17:08.296542Z",
     "iopub.status.busy": "2021-03-27T12:17:08.295701Z",
     "iopub.status.idle": "2021-03-27T12:17:20.451112Z",
     "shell.execute_reply": "2021-03-27T12:17:20.450392Z"
    },
    "papermill": {
     "duration": 12.175018,
     "end_time": "2021-03-27T12:17:20.451305",
     "exception": false,
     "start_time": "2021-03-27T12:17:08.276287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/zomato-bangalore-dataset/zomato.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m zomato_real\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/zomato-bangalore-dataset/zomato.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m zomato_real\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/zomato-bangalore-dataset/zomato.csv'"
     ]
    }
   ],
   "source": [
    "zomato_real=pd.read_csv(\"../input/zomato-bangalore-dataset/zomato.csv\")\n",
    "zomato_real.head() # prints the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010143,
     "end_time": "2021-03-27T12:17:20.473242",
     "exception": false,
     "start_time": "2021-03-27T12:17:20.463099",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now the next step is data cleaning and feature engineering for this step we need to do a lot of stuff with the data such as:\n",
    "\n",
    "1. Deleting Unnecessary Columns\n",
    "2. Removing the Duplicates\n",
    "3. Remove the NaN values from the dataset\n",
    "4. Changing the column names\n",
    "5. Data Transformations\n",
    "6. Data Cleaning\n",
    "7. Adjust the column names\n",
    "Now, letâ€™s perform all the above steps in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T12:17:20.522672Z",
     "iopub.status.busy": "2021-03-27T12:17:20.520337Z",
     "iopub.status.idle": "2021-03-27T12:19:00.382437Z",
     "shell.execute_reply": "2021-03-27T12:19:00.381775Z"
    },
    "papermill": {
     "duration": 99.898883,
     "end_time": "2021-03-27T12:19:00.382606",
     "exception": false,
     "start_time": "2021-03-27T12:17:20.483723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Deleting Unnnecessary Columns\n",
    "zomato=zomato_real.drop(['url','dish_liked','phone'],axis=1) #Dropping the column \"dish_liked\", \"phone\", \"url\" and saving the new dataset as \"zomato\"\n",
    "\n",
    "#Removing the Duplicates\n",
    "zomato.duplicated().sum()\n",
    "zomato.drop_duplicates(inplace=True)\n",
    "\n",
    "#Remove the NaN values from the dataset\n",
    "zomato.isnull().sum()\n",
    "zomato.dropna(how='any',inplace=True)\n",
    "\n",
    "#Changing the column names\n",
    "zomato = zomato.rename(columns={'approx_cost(for two people)':'cost','listed_in(type)':'type', 'listed_in(city)':'city'})\n",
    "\n",
    "#Some Transformations\n",
    "zomato['cost'] = zomato['cost'].astype(str) #Changing the cost to string\n",
    "zomato['cost'] = zomato['cost'].apply(lambda x: x.replace(',','.')) #Using lambda function to replace ',' from cost\n",
    "zomato['cost'] = zomato['cost'].astype(float)\n",
    "#Removing '/5' from Rates\n",
    "zomato = zomato.loc[zomato.rate !='NEW']\n",
    "zomato = zomato.loc[zomato.rate !='-'].reset_index(drop=True)\n",
    "remove_slash = lambda x: x.replace('/5', '') if type(x) == np.str else x\n",
    "zomato.rate = zomato.rate.apply(remove_slash).str.strip().astype('float')\n",
    "\n",
    "# Adjust the column names\n",
    "zomato.name = zomato.name.apply(lambda x:x.title())\n",
    "zomato.online_order.replace(('Yes','No'),(True, False),inplace=True)\n",
    "zomato.book_table.replace(('Yes','No'),(True, False),inplace=True)\n",
    "\n",
    "## Computing Mean Rating\n",
    "restaurants = list(zomato['name'].unique())\n",
    "zomato['Mean Rating'] = 0\n",
    "\n",
    "for i in range(len(restaurants)):\n",
    "    zomato['Mean Rating'][zomato['name'] == restaurants[i]] = zomato['rate'][zomato['name'] == restaurants[i]].mean()\n",
    "    \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (1,5))\n",
    "zomato[['Mean Rating']] = scaler.fit_transform(zomato[['Mean Rating']]).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010468,
     "end_time": "2021-03-27T12:19:00.404875",
     "exception": false,
     "start_time": "2021-03-27T12:19:00.394407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now the next step is to perform some text preprocessing steps which include:\n",
    "\n",
    "1. Lower casing\n",
    "2. Removal of Punctuations\n",
    "3. Removal of Stopwords\n",
    "4. Removal of URLs\n",
    "5. Spelling correction\n",
    "\n",
    "Now letâ€™s perform the above text preprocessing steps on the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T12:19:00.443348Z",
     "iopub.status.busy": "2021-03-27T12:19:00.435366Z",
     "iopub.status.idle": "2021-03-27T12:20:07.232610Z",
     "shell.execute_reply": "2021-03-27T12:20:07.233136Z"
    },
    "papermill": {
     "duration": 66.8175,
     "end_time": "2021-03-27T12:20:07.233321",
     "exception": false,
     "start_time": "2021-03-27T12:19:00.415821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_list</th>\n",
       "      <th>cuisines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19691</th>\n",
       "      <td>rated 40 ratedn hi allnni visited place friend...</td>\n",
       "      <td>South Indian, North Indian, Chinese, Street Food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35018</th>\n",
       "      <td>rated 40 ratedn got friday nightnot crowded go...</td>\n",
       "      <td>Mediterranean, Italian, Asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22624</th>\n",
       "      <td>rated 10 ratedn bad experience air conditionin...</td>\n",
       "      <td>Mexican, Continental, Italian, Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32489</th>\n",
       "      <td>rated 10 ratedn packed drinage food delivered ...</td>\n",
       "      <td>North Indian, Biryani, Chinese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38093</th>\n",
       "      <td>rated 40 ratedn hello regular adda week visit ...</td>\n",
       "      <td>Cafe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            reviews_list  \\\n",
       "19691  rated 40 ratedn hi allnni visited place friend...   \n",
       "35018  rated 40 ratedn got friday nightnot crowded go...   \n",
       "22624  rated 10 ratedn bad experience air conditionin...   \n",
       "32489  rated 10 ratedn packed drinage food delivered ...   \n",
       "38093  rated 40 ratedn hello regular adda week visit ...   \n",
       "\n",
       "                                               cuisines  \n",
       "19691  South Indian, North Indian, Chinese, Street Food  \n",
       "35018                     Mediterranean, Italian, Asian  \n",
       "22624            Mexican, Continental, Italian, Chinese  \n",
       "32489                    North Indian, Biryani, Chinese  \n",
       "38093                                              Cafe  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lower Casing\n",
    "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].str.lower()\n",
    "\n",
    "## Removal of Puctuations\n",
    "import string\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "\n",
    "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "## Removal of Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_stopwords(text))\n",
    "\n",
    "## Removal of URLS\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "zomato[\"reviews_list\"] = zomato[\"reviews_list\"].apply(lambda text: remove_urls(text))\n",
    "\n",
    "zomato[['reviews_list', 'cuisines']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T12:20:07.268026Z",
     "iopub.status.busy": "2021-03-27T12:20:07.267264Z",
     "iopub.status.idle": "2021-03-27T12:20:07.290198Z",
     "shell.execute_reply": "2021-03-27T12:20:07.289458Z"
    },
    "papermill": {
     "duration": 0.045971,
     "end_time": "2021-03-27T12:20:07.290351",
     "exception": false,
     "start_time": "2021-03-27T12:20:07.244380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# RESTAURANT NAMES:\n",
    "restaurant_names = list(zomato['name'].unique())\n",
    "def get_top_words(column, top_nu_of_words, nu_of_word):\n",
    "    vec = CountVectorizer(ngram_range= nu_of_word, stop_words='english')\n",
    "    bag_of_words = vec.fit_transform(column)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_nu_of_words]\n",
    "    \n",
    "zomato=zomato.drop(['address','rest_type', 'type', 'menu_item', 'votes'],axis=1)\n",
    "import pandas\n",
    "\n",
    "# Randomly sample 60% of your dataframe\n",
    "df_percent = zomato.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010948,
     "end_time": "2021-03-27T12:20:07.312827",
     "exception": false,
     "start_time": "2021-03-27T12:20:07.301879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TF-IDF Vectorization\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) vectors for each document. This will give you a matrix where each column represents a word in the general vocabulary (all words that appear in at least one document) and each column represents a restaurant, as before.\n",
    "\n",
    "TF-IDF is the statistical method of assessing the meaning of a word in a given document. Now, I will use the TF-IDF vectorization on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T12:20:07.351984Z",
     "iopub.status.busy": "2021-03-27T12:20:07.344461Z",
     "iopub.status.idle": "2021-03-27T12:22:33.019880Z",
     "shell.execute_reply": "2021-03-27T12:22:33.019157Z"
    },
    "papermill": {
     "duration": 145.695113,
     "end_time": "2021-03-27T12:22:33.020088",
     "exception": false,
     "start_time": "2021-03-27T12:20:07.324975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_percent.set_index('name', inplace=True)\n",
    "indices = pd.Series(df_percent.index)\n",
    "\n",
    "# Creating tf-idf matrix\n",
    "tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(df_percent['reviews_list'])\n",
    "\n",
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01119,
     "end_time": "2021-03-27T12:22:33.043407",
     "exception": false,
     "start_time": "2021-03-27T12:22:33.032217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now the last step for creating a Restaurant Recommendation System is to write a function that will recommend restaurants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-27T12:22:33.086078Z",
     "iopub.status.busy": "2021-03-27T12:22:33.085371Z",
     "iopub.status.idle": "2021-03-27T12:22:33.443865Z",
     "shell.execute_reply": "2021-03-27T12:22:33.445083Z"
    },
    "papermill": {
     "duration": 0.389456,
     "end_time": "2021-03-27T12:22:33.445357",
     "exception": false,
     "start_time": "2021-03-27T12:22:33.055901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 10 RESTAURANTS LIKE Pai Vihar WITH SIMILAR REVIEWS: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisines</th>\n",
       "      <th>Mean Rating</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atithi</th>\n",
       "      <td>North Indian, Chinese, Street Food</td>\n",
       "      <td>3.63</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atithi</th>\n",
       "      <td>North Indian</td>\n",
       "      <td>3.63</td>\n",
       "      <td>750.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Samosa Singh</th>\n",
       "      <td>Street Food, Fast Food, Rolls, Desserts</td>\n",
       "      <td>3.60</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Magix'S Parattha Roll</th>\n",
       "      <td>Fast Food, North Indian, Chinese, Mughlai, Rolls</td>\n",
       "      <td>3.52</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prasiddhi Food Corner</th>\n",
       "      <td>Fast Food, North Indian, South Indian</td>\n",
       "      <td>3.45</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shrusti Coffee</th>\n",
       "      <td>Cafe, South Indian</td>\n",
       "      <td>3.45</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shanthi Sagar</th>\n",
       "      <td>South Indian, North Indian, Chinese, Street Fo...</td>\n",
       "      <td>3.44</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mayura Sagar</th>\n",
       "      <td>Chinese, North Indian, South Indian</td>\n",
       "      <td>3.32</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vasanth Vihar - Since 1965</th>\n",
       "      <td>South Indian, Street Food</td>\n",
       "      <td>3.32</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marwa Restaurant</th>\n",
       "      <td>North Indian, Chinese, Fast Food, BBQ</td>\n",
       "      <td>3.19</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     cuisines  \\\n",
       "Atithi                                     North Indian, Chinese, Street Food   \n",
       "Atithi                                                           North Indian   \n",
       "Samosa Singh                          Street Food, Fast Food, Rolls, Desserts   \n",
       "Magix'S Parattha Roll        Fast Food, North Indian, Chinese, Mughlai, Rolls   \n",
       "Prasiddhi Food Corner                   Fast Food, North Indian, South Indian   \n",
       "Shrusti Coffee                                             Cafe, South Indian   \n",
       "Shanthi Sagar               South Indian, North Indian, Chinese, Street Fo...   \n",
       "Mayura Sagar                              Chinese, North Indian, South Indian   \n",
       "Vasanth Vihar - Since 1965                          South Indian, Street Food   \n",
       "Marwa Restaurant                        North Indian, Chinese, Fast Food, BBQ   \n",
       "\n",
       "                            Mean Rating   cost  \n",
       "Atithi                             3.63  800.0  \n",
       "Atithi                             3.63  750.0  \n",
       "Samosa Singh                       3.60  200.0  \n",
       "Magix'S Parattha Roll              3.52  400.0  \n",
       "Prasiddhi Food Corner              3.45  200.0  \n",
       "Shrusti Coffee                     3.45  150.0  \n",
       "Shanthi Sagar                      3.44  400.0  \n",
       "Mayura Sagar                       3.32  250.0  \n",
       "Vasanth Vihar - Since 1965         3.32  150.0  \n",
       "Marwa Restaurant                   3.19  600.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def recommend(name, cosine_similarities = cosine_similarities):\n",
    "    \n",
    "    # Create a list to put top restaurants\n",
    "    recommend_restaurant = []\n",
    "    \n",
    "    # Find the index of the hotel entered\n",
    "    idx = indices[indices == name].index[0]\n",
    "    \n",
    "    # Find the restaurants with a similar cosine-sim value and order them from bigges number\n",
    "    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending=False)\n",
    "    \n",
    "    # Extract top 30 restaurant indexes with a similar cosine-sim value\n",
    "    top30_indexes = list(score_series.iloc[0:31].index)\n",
    "    \n",
    "    # Names of the top 30 restaurants\n",
    "    for each in top30_indexes:\n",
    "        recommend_restaurant.append(list(df_percent.index)[each])\n",
    "    \n",
    "    # Creating the new data set to show similar restaurants\n",
    "    df_new = pd.DataFrame(columns=['cuisines', 'Mean Rating', 'cost'])\n",
    "    \n",
    "    # Create the top 30 similar restaurants with some of their columns\n",
    "    for each in recommend_restaurant:\n",
    "        df_new = df_new.append(pd.DataFrame(df_percent[['cuisines','Mean Rating', 'cost']][df_percent.index == each].sample()))\n",
    "    \n",
    "    # Drop the same named restaurants and sort only the top 10 by the highest rating\n",
    "    df_new = df_new.drop_duplicates(subset=['cuisines','Mean Rating', 'cost'], keep=False)\n",
    "    df_new = df_new.sort_values(by='Mean Rating', ascending=False).head(10)\n",
    "    \n",
    "    print('TOP %s RESTAURANTS LIKE %s WITH SIMILAR REVIEWS: ' % (str(len(df_new)), name))\n",
    "    \n",
    "    return df_new\n",
    "recommend('Pai Vihar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 335.697414,
   "end_time": "2021-03-27T12:22:34.731535",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-27T12:16:59.034121",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
